defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

seed: 0
ds_path: null
tokens_params_ratio: 20 # chinchilla scaling
num_tokens_train: null
log_every_tokens: 1_000_000
num_tokens_valid: 20_000_000
wandb_project: 'picodo'
wandb_mode: 'disabled'
run_name: null
num_tp_devices: 1 # optional tensor parallelism

model:
  D: null # model/embed/qkv dim
  L: null # num. block layers
  H: 128 # head dimension
  F: ${mul:4, ${model.D}} # FF inner dimension = 4 x embed dim.
  N: ${floordiv:${model.D}, ${model.H}} # num. attention heads
  T: null # context/sequence length
  V: null # vocab size -> must match dataset tokenizer!
  activ_dtype: 'bfloat16'
  remat: false # gradient rematerialization (chekpointing)
  use_flash_attn: true
  
opt:
  batch_size: 8
  microbatch_size: ${opt.batch_size}
  grad_acc_steps: ${floordiv:${opt.batch_size}, ${opt.microbatch_size}}
  peak_lr: 0.002
  warmup_frac: 0.05
  b1: 0.9
  b2: 0.999
  weight_decay: 0.02
